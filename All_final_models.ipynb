{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acbaa70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import optuna\n",
    "\n",
    "# sklearn imports\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, precision_score, recall_score, f1_score, roc_curve, auc\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Other libraries\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from xgboost import XGBClassifier\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deda9d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "file_path = \"Clickstream_data.csv\"\n",
    "expanded_file_path = os.path.expanduser(file_path)\n",
    "data = pd.read_csv(expanded_file_path)\n",
    "\n",
    "# extreme values will be removed based on the exploratory analysis\n",
    "features_to_clean = {\n",
    "    'ProductRelated_Duration': 2,\n",
    "    'Informational': 1,\n",
    "    'Administrative_Duration': 1,\n",
    "    'ProductRelated': 2,\n",
    "    'PageValues': 2\n",
    "}\n",
    "\n",
    "# Create a copy of the DataFrame for modifications\n",
    "data_modified = data.copy()\n",
    "\n",
    "# Remove the rows with the outliers in the specified features\n",
    "for feature, count in features_to_clean.items():\n",
    "    for _ in range(count):\n",
    "        max_value_index = data_modified[feature].idxmax()\n",
    "        data_modified.drop(max_value_index, inplace=True)\n",
    "\n",
    "data_modified\n",
    "# Rename the dataset name from data_modified to online_csd which is short for Online Clicksteam data\n",
    "Online_csd = data_modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d0c740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1- Data Preprocessing :\n",
    "# A- One-hot encoding \n",
    "# Specify the columns to one-hot encode for categorical variables\n",
    "Online_csd_coded = pd.get_dummies(Online_csd, columns=['Month', 'VisitorType'], drop_first=True)\n",
    "\n",
    "# 2- Label encoding for Weekend and Revenue\n",
    "le = LabelEncoder()\n",
    "Online_csd_coded['Weekend'] = le.fit_transform(Online_csd_coded['Weekend'])\n",
    "Online_csd_coded['Revenue'] = le.fit_transform(Online_csd_coded['Revenue'])\n",
    "\n",
    "# 3- Splitting dependent and independent variables(columns)\n",
    "features = Online_csd_coded.drop(['Revenue'], axis = 1)\n",
    "target = Online_csd_coded['Revenue']\n",
    "\n",
    "# checking the shapes\n",
    "# print(\"Shape of the features: \", features.shape)\n",
    "# print(\"Shape of the target (Revenue) : \", target.shape)\n",
    "\n",
    "# 4- Stratified sampling to split the data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, target, test_size =  0.2, random_state = 42, stratify=target)\n",
    "\n",
    "# 5- Applying SMOTE oversampling to the training data\n",
    "smote = SMOTE(random_state=42)\n",
    "x_train_resampled, y_train_resampled = smote.fit_resample(x_train, y_train)\n",
    "\n",
    "# 6- Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train_resampled)\n",
    "x_test_scaled = scaler.transform(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb1d162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bernoulli Naive Bayes model\n",
    "# Initialize StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize the Bernoulli Naive Bayes model\n",
    "bnb_model = BernoulliNB()\n",
    "\n",
    "f1_scores = []\n",
    "\n",
    "# Perform cross-validation \n",
    "for train_index, val_index in skf.split(x_train_resampled, y_train_resampled):\n",
    "    x_train_fold, x_val_fold = x_train_resampled.iloc[train_index], x_train_resampled.iloc[val_index]\n",
    "    y_train_fold, y_val_fold = y_train_resampled.iloc[train_index], y_train_resampled.iloc[val_index]\n",
    "\n",
    "    # Fit the model on the training fold\n",
    "    bnb_model.fit(x_train_fold, y_train_fold)\n",
    "\n",
    "    # Predict on the validation fold\n",
    "    y_pred_fold = bnb_model.predict(x_val_fold)\n",
    "\n",
    "    # Calculate the F1 score and append to the list\n",
    "    f1 = f1_score(y_val_fold, y_pred_fold)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "# Calculate the average F1 score across all folds\n",
    "avg_f1_score = np.mean(f1_scores)\n",
    "print(\"Average F1 Score (Cross-Validated):\", avg_f1_score)\n",
    "\n",
    "# Fit the model on the entire training set\n",
    "bnb_model.fit(x_train_resampled, y_train_resampled)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_naive = bnb_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22245aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set \n",
    "print(\"Bernoulli Naive Bayes Test Metrics:\")\n",
    "# Generate a classification report\n",
    "report = classification_report(y_test, y_pred_naive, digits=3)\n",
    "print('Classification Report:\\n', report)\n",
    "\n",
    "# Create a confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_naive)\n",
    "print('Confusion Matrix:\\n', cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d1e92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix for Naive Bayes\n",
    "cm = confusion_matrix(y_test, y_pred_naive)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap='Blues')\n",
    "plt.title(\"Confusion Matrix - Naive Bayes\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685d983e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SVM Model\n",
    "# Define the objective function for hyperparameter optimization with StratifiedKFold\n",
    "def objective_svm(trial):\n",
    "    C = trial.suggest_float('C', 0.01, 20, log=True)  # Using a logarithmic scale for C\n",
    "    gamma = trial.suggest_categorical('gamma', ['scale', 'auto'])  # Exploring 'scale' and 'auto' for gamma\n",
    "    kernel = trial.suggest_categorical('kernel', ['linear', 'rbf', 'poly', 'sigmoid'])  # Including 'sigmoid'\n",
    "    degree = trial.suggest_int('degree', 2, 5) if kernel == 'poly' else 3  # For 'poly' kernel\n",
    "    coef0 = trial.suggest_float('coef0', 0.0, 10.0) if kernel in ['poly', 'sigmoid'] else 0.0  # For 'poly' and 'sigmoid'\n",
    "    class_weight = trial.suggest_categorical('class_weight', [None, 'balanced'])  # Class weight options\n",
    "\n",
    "    svm_model = SVC(C=C, gamma=gamma, kernel=kernel, degree=degree, coef0=coef0, class_weight=class_weight, probability=True, random_state=42)\n",
    "\n",
    "    # Initialize StratifiedKFold\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    # List to store f1 scores for each fold\n",
    "    f1_scores = []\n",
    "\n",
    "    # Perform cross-validation manually\n",
    "    for train_index, val_index in skf.split(x_train_scaled, y_train_resampled):\n",
    "        x_train_fold, x_val_fold = x_train_scaled[train_index], x_train_scaled[val_index]\n",
    "        y_train_fold, y_val_fold = y_train_resampled[train_index], y_train_resampled[val_index]\n",
    "\n",
    "        # Fit the model on the training fold\n",
    "        svm_model.fit(x_train_fold, y_train_fold)\n",
    "\n",
    "        # Predict on the validation fold\n",
    "        y_pred_fold = svm_model.predict(x_val_fold)\n",
    "\n",
    "        # Calculate the F1 score and append to the list\n",
    "        f1 = f1_score(y_val_fold, y_pred_fold)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    # Return the average F1 score across all folds\n",
    "    return np.mean(f1_scores)\n",
    "\n",
    "# Create and optimize an Optuna study\n",
    "study_svm = optuna.create_study(direction=\"maximize\")\n",
    "study_svm.optimize(objective_svm, n_trials=40)\n",
    "\n",
    "# Print best hyperparameters\n",
    "print(\"Best Hyperparameters:\", study_svm.best_trial.params)\n",
    "\n",
    "# Train the final model with the best hyperparameters\n",
    "best_params = study_svm.best_trial.params\n",
    "best_svm_model = SVC(**best_params, probability=True, random_state=42)\n",
    "best_svm_model.fit(x_train_scaled, y_train_resampled)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_svm = best_svm_model.predict(x_test_scaled)\n",
    "y_prob = best_svm_model.predict_proba(x_test_scaled)[:, 1]  # Get probabilities for the positive class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51ac9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set \n",
    "# Print test metrics\n",
    "print(\"SVM Test Metrics:\")\n",
    "# Print classification report and confusion matrix\n",
    "report = classification_report(y_test, y_pred_svm, digits=3)\n",
    "print('Classification Report:\\n', report)\n",
    "cm = confusion_matrix(y_test, y_pred_svm)\n",
    "print('Confusion Matrix:\\n', cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf6f854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix for SVM\n",
    "cm = confusion_matrix(y_test, y_pred_svm)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap='Blues')\n",
    "plt.title(\"Confusion Matrix - SVM\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa12c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## XGBoost\n",
    "# Define the objective function for hyperparameter optimization with StratifiedKFold\n",
    "def objective_xgb(trial):\n",
    "    # Hyperparameters to be tuned by Optuna\n",
    "    xgb_params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 300, step=20),\n",
    "        'max_depth': trial.suggest_int('max_depth', 2, 30),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 1, step=0.01),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 3)\n",
    "    }\n",
    "\n",
    "    # Initialize StratifiedKFold\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    # List to store f1 scores for each fold\n",
    "    f1_scores = []\n",
    "\n",
    "    # Perform cross-validation\n",
    "    for train_index, val_index in skf.split(x_train_resampled, y_train_resampled):\n",
    "        x_train_fold, x_val_fold = x_train_resampled.iloc[train_index], x_train_resampled.iloc[val_index]\n",
    "        y_train_fold, y_val_fold = y_train_resampled.iloc[train_index], y_train_resampled.iloc[val_index]\n",
    "\n",
    "        # Create the XGBoost model\n",
    "        xgb_model = XGBClassifier(**xgb_params, random_state=42)\n",
    "\n",
    "        # Fit the model on the training fold\n",
    "        xgb_model.fit(x_train_fold, y_train_fold)\n",
    "\n",
    "        # Predict on the validation fold\n",
    "        y_pred_fold = xgb_model.predict(x_val_fold)\n",
    "\n",
    "        # Calculate the F1 score and append to the list\n",
    "        f1 = f1_score(y_val_fold, y_pred_fold, average='binary')\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    # Return the average F1 score across all folds\n",
    "    return np.mean(f1_scores)\n",
    "\n",
    "# Create and optimize an Optuna study\n",
    "study_xgb = optuna.create_study(direction=\"maximize\")\n",
    "study_xgb.optimize(objective_xgb, n_trials=40)\n",
    "\n",
    "# Print best hyperparameters\n",
    "print(\"Best Hyperparameters:\", study_xgb.best_trial.params)\n",
    "\n",
    "# Train final model with best hyperparameters\n",
    "best_params = study_xgb.best_trial.params\n",
    "final_xgb_model = XGBClassifier(**best_params, random_state=42)\n",
    "final_xgb_model.fit(x_train_resampled, y_train_resampled)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_xgb = final_xgb_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774c6abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "# Print test metrics\n",
    "print(\"XGBoost Test Metrics:\")\n",
    "# Print classification report and confusion matrix\n",
    "report = classification_report(y_test, y_pred_xgb, digits=3)\n",
    "print('Classification Report:\\n', report)\n",
    "cm = confusion_matrix(y_test, y_pred_xgb)\n",
    "print('Confusion Matrix:\\n', cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7921df6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix for Random Forest\n",
    "cm = confusion_matrix(y_test, y_pred_xgb)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix - XGBoost\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad55936",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Random Forest\n",
    "# Define the objective function for hyperparameter optimization with StratifiedKFold\n",
    "def objective_rf(trial):\n",
    "    # Hyperparameters to be tuned by Optuna\n",
    "    rf_params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 300, step=20),\n",
    "        'max_depth': trial.suggest_int('max_depth', 2, 30),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 10, step=2),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 2, 10, step=2),\n",
    "        'bootstrap': trial.suggest_categorical('bootstrap', [True, False])\n",
    "    }\n",
    "\n",
    "    # Create the Random Forest model\n",
    "    rf_model = RandomForestClassifier(**rf_params, random_state=42)\n",
    "\n",
    "    # Initialize StratifiedKFold\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    # List to store f1 scores for each fold\n",
    "    f1_scores = []\n",
    "\n",
    "    # Perform cross-validation\n",
    "    for train_index, val_index in skf.split(x_train_resampled, y_train_resampled):\n",
    "        x_train_fold, x_val_fold = x_train_resampled.iloc[train_index], x_train_resampled.iloc[val_index]\n",
    "        y_train_fold, y_val_fold = y_train_resampled.iloc[train_index], y_train_resampled.iloc[val_index]\n",
    "\n",
    "        # Fit the model on the training fold\n",
    "        rf_model.fit(x_train_fold, y_train_fold)\n",
    "\n",
    "        # Predict on the validation fold\n",
    "        y_pred_fold = rf_model.predict(x_val_fold)\n",
    "\n",
    "        # Calculate the F1 score and append to the list\n",
    "        f1 = f1_score(y_val_fold, y_pred_fold, average='weighted')\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    # Return the average F1 score across all folds\n",
    "    return np.mean(f1_scores)\n",
    "\n",
    "# Create and optimize an Optuna study\n",
    "study_rf = optuna.create_study(direction=\"maximize\")\n",
    "study_rf.optimize(objective_rf, n_trials=40)\n",
    "\n",
    "# Print best hyperparameters\n",
    "print(\"Best Hyperparameters:\", study_rf.best_trial.params)\n",
    "\n",
    "# Train final model with best hyperparameters\n",
    "best_params = study_rf.best_trial.params\n",
    "final_rf_model = RandomForestClassifier(**best_params, random_state=42)\n",
    "final_rf_model.fit(x_train_resampled, y_train_resampled)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_rf = final_rf_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbc4526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set using different metrics\n",
    "# Print test metrics\n",
    "print(\"Random Forest Test Metrics:\")\n",
    "# Print classification report and confusion matrix\n",
    "report = classification_report(y_test, y_pred_rf, digits=3)\n",
    "print('Classification Report:\\n', report)\n",
    "cm = confusion_matrix(y_test, y_pred_rf)\n",
    "print('Confusion Matrix:\\n', cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc136a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix for Random Forest\n",
    "cm = confusion_matrix(y_test, y_pred_rf)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix - Random Forest\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90decd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tabnet\n",
    "# Initialize Stratified 5-Fold Cross-Validation\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "fixed_learning_rate = 0.001\n",
    "\n",
    "def objective_tabnet(trial):\n",
    "    # Hyperparameters to be tuned by Optuna\n",
    "    mask_type = trial.suggest_categorical(\"mask_type\", [\"sparsemax\", \"entmax\"])\n",
    "    n_d = trial.suggest_int(\"n_d\", 8, 64)\n",
    "    n_a = trial.suggest_int(\"n_a\", 8, 64)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [64, 128, 256])\n",
    "    virtual_batch_size = trial.suggest_categorical(\"virtual_batch_size\", [32, 64, 128])\n",
    "\n",
    "    f1_scores = []\n",
    "\n",
    "    for train_index, val_index in skf.split(x_train_scaled, y_train_resampled):\n",
    "        x_train_fold, x_val_fold = x_train_scaled[train_index], x_train_scaled[val_index]\n",
    "        y_train_fold, y_val_fold = y_train_resampled[train_index], y_train_resampled[val_index]\n",
    "\n",
    "        # Create the TabNet model with the fixed learning rate\n",
    "        model = TabNetClassifier(optimizer_fn=torch.optim.Adam,\n",
    "                                 optimizer_params=dict(lr=fixed_learning_rate), \n",
    "                                 mask_type=mask_type, \n",
    "                                 n_d=n_d, \n",
    "                                 n_a=n_a)\n",
    "\n",
    "        # Train the model\n",
    "        model.fit(X_train=x_train_fold, y_train=y_train_fold,\n",
    "                  eval_set=[(x_val_fold, y_val_fold)],\n",
    "                  eval_metric=['auc'],\n",
    "                  max_epochs=50,  # You can adjust this\n",
    "                  patience=10,  # And this\n",
    "                  batch_size=batch_size,\n",
    "                  virtual_batch_size=virtual_batch_size,\n",
    "                  num_workers=0,\n",
    "                  drop_last=False)\n",
    "\n",
    "        # Evaluate the model\n",
    "        preds = model.predict(x_val_fold)\n",
    "        f1 = f1_score(y_val_fold, preds, average='weighted')\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    return np.mean(f1_scores)\n",
    "\n",
    "\n",
    "# Create and optimize an Optuna\n",
    "study_tabnet = optuna.create_study(direction=\"maximize\")\n",
    "study_tabnet.optimize(objective_tabnet, n_trials=40)\n",
    "\n",
    "# Best hyperparameters\n",
    "best_params = study_tabnet.best_trial.params\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# Train final model with best hyperparameters\n",
    "final_model = TabNetClassifier(optimizer_fn=torch.optim.Adam,\n",
    "                               optimizer_params=dict(lr=0.001),\n",
    "                               mask_type=best_params['mask_type'],\n",
    "                               n_d=best_params['n_d'],\n",
    "                               n_a=best_params['n_a'])\n",
    "\n",
    "final_model.fit(X_train=x_train_scaled, y_train=y_train_resampled,\n",
    "                eval_set=[(x_train_scaled, y_train_resampled)],\n",
    "                eval_metric=['auc'],\n",
    "                max_epochs=50,  # Adjust as needed\n",
    "                patience=10,  # Adjust as needed\n",
    "                batch_size=best_params['batch_size'],\n",
    "                virtual_batch_size=best_params['virtual_batch_size'],\n",
    "                num_workers=0,\n",
    "                drop_last=False)\n",
    "# Evaluate on the test set\n",
    "y_pred_tabnet = final_model.predict(x_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143d2b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print test metrics\n",
    "print(\"TabNet Test Metrics:\")\n",
    "# Print classification report and confusion matrix\n",
    "report = classification_report(y_test, y_pred_tabnet, digits=3)\n",
    "print('Classification Report:\\n', report)\n",
    "cm = confusion_matrix(y_test, y_pred_tabnet)\n",
    "print('Confusion Matrix:\\n', cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89df79b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix for tabnet\n",
    "cm = confusion_matrix(y_test, y_pred_tabnet)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix - TabNet\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c62d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance using best performer (Random Forest)\n",
    "# Fitting a Random Forest Classifier for feature importance\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "rf_classifier.fit(x_train, y_train)\n",
    "\n",
    "importances = rf_classifier.feature_importances_\n",
    "feature_names = features.columns\n",
    "sorted_indices = np.argsort(importances)[::-1]\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(\"Feature Importances\")\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, x_train.shape[1]))\n",
    "plt.bar(range(x_train.shape[1]), importances[sorted_indices], color=colors, align='center')\n",
    "plt.xticks(range(x_train.shape[1]), feature_names[sorted_indices], rotation=90)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
